<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Web Audio | Ear to the Bits]]></title>
  <link href="http:///blog/categories/web-audio/atom.xml" rel="self"/>
  <link href="http:///"/>
  <updated>2014-12-06T16:57:12-05:00</updated>
  <id>http:///</id>
  <author>
    <name><![CDATA[Cassie Tarakajian]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Web Audio Timing Tutorial]]></title>
    <link href="http:///blog/2014/12/02/web-audio-timing-tutorial/"/>
    <updated>2014-12-02T22:56:24-05:00</updated>
    <id>http:///blog/2014/12/02/web-audio-timing-tutorial</id>
    <content type="html"><![CDATA[<p>My last post was a delirious declaration that I had finally made something with Web Audio. <a href="http://web-audio-sequencer.herokuapp.com/">Here it is</a>, a drum sequencer in the browser. Now I want to explain how I did it. First, what is Web Audio, and why do we need it?</p>

<p>The Web Audio API is a versatile system for controlling audio in the web. It does this inside an <strong>audio context</strong>, which you declare as follows:</p>

<pre><code class="javascript">var context = new AudioContext();
</code></pre>

<p>You then create sources, either by loading a song or sample via an AJAX request (though you can&rsquo;t use jQuery since it doesn&rsquo;t support responses of type &lsquo;arrayBuffer&rsquo;:</p>

<pre><code class="javascript">var songBuffer = null;

//for multiple browser support
window.AudioContext = window.AudioContext || window.webkitAudioContext;
var context = new AudioContext();
function loadSong(url) { // the url can be full path or relative
  var request = new XMLHttpRequest();
  request.open('GET', url, true);
  request.responseType = 'arraybuffer';

  // Decode asynchronously
  request.onload = function() {
    context.decodeAudioData(request.response, function(buffer) {
      songBuffer = buffer;
    }, onError);
  }
  request.send();
}
</code></pre>

<p>Once you have sources, you route sounds through a series of <strong>audio nodes</strong> to add effects, such as reverb, filtering, and compression. It is also <strong>modular</strong>, meaning it is simple to change the routes to add or remove effects. It is analogous to physical synthesizers and modular synthesizers. The last node in the <strong>audio routing graph</strong> is the destination node, which is responsible for actually <em>playing</em> the sounds. Here&rsquo;s a routing example:</p>

<pre><code class="javascript">function playSound(buffer) {
  // creates a sound source from buffer just loaded
  var source = context.createBufferSource(); 
  source.buffer = buffer;                    // tell the source which sound to play
  source.connect(context.destination);       // connect source to context's destination (the speakers)
  source.start(0);                           // play the source now
}
</code></pre>

<p>Cool, so now we can play audio. As soon as I figured out how to do this, the first thing I wanted to learn how to do is how to sequence audio events, because that&rsquo;s how you making music: you organize sounds in time. This is a deep, dark rabbit hole. On the <a href="http://www.html5rocks.com/en/tutorials/webaudio/intro/">introduction to Web Audio tutorial</a>, they have an example which synchronizing to the Javascript clock. But this is bad. <em>Very</em> bad. The Javascript clock is not precise, and therefore if you synchronize audio events to it, there will be noticeable latency and stuttering. You must synchronize to the Web Audio clock, which is a signal from an actual hardware crystal (and therefore very precise).</p>

<p>Another problem is scheduling. Web Audio has no built in scheduler. Therefore, you have to build your own (or use a library that has one, for example, <a href="https://www.npmjs.org/package/waaclock">WAAClock</a> or <a href="https://github.com/CreateJS/SoundJS">SoundJS</a>). You also have to think how far ahead you want to schedule your audio events. For example, if you have the code (which is synchronized to the Javascript clock)</p>

<pre><code class="javascript">for (var bar = 0; bar &lt; 2; bar++) {
  var time = startTime + bar * 8 * eighthNoteTime;

  // Play the hi-hat every eighth note.
  for (var i = 0; i &lt; 8; ++i) {
    playSound(hihat, time + i * eighthNoteTime);
  }
}
</code></pre>

<p>and you want to change tempo, what do you do? You can&rsquo;t unschedule the notes once you&rsquo;ve told them to play. You would have to add a gain node to control the volume, turn off the volume on the now off-tempo loop, and then create a <em>new</em> loop with the new correct tempo. And ad infinitum every time you want to change the tempo again.</p>

<p>That&rsquo;s awful, and there is a better way. After reading <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/">A Tale of Two Clocks</a> a few times until it finally made sense. The way to make a good scheduler is to schedule one note ahead of time, and to synchronize it to the Web Audio clock. Unfortunately, it&rsquo;s easier said than done. Your scheduler function ends up looking something like this:</p>

<pre><code class="javascript">while (nextNoteTime &lt; audioContext.currentTime + scheduleAheadTime ) {
  scheduleNote( current16thNote, nextNoteTime );
  nextNote();
}
</code></pre>

<p>But scheduleAheadTime and nextNoteTime have to be tweaked according to your use case. In my case, I ended up using the same parameters as used in the <a href="http://chromium.googlecode.com/svn/trunk/samples/audio/shiny-drum-machine.html">Google Shiny Drum Machine</a> and it seems to be okay. Let&rsquo;s look at my code for the core scheduling functionality:</p>

<pre><code class="javascript">function handlePlay(event) {
    noteTime = 0.0;
    startTime = context.currentTime + 0.005;
    schedule();
}


function schedule() {
  var currentTime = context.currentTime;

  // The sequence starts at startTime, so normalize currentTime so that it's 0 at the start of the sequence.
  currentTime -= startTime;

  while (noteTime &lt; currentTime + 0.200) {
    var contextPlayTime = noteTime + startTime;

    //Insert playing notes here

    //Insert draw stuff here

    advanceNote();

  }
  timeoutId = setTimeout("schedule()", 0);
}

function advanceNote() {
    // Setting tempo to 60 BPM just for now
    var tempo = 60.0;
    var secondsPerBeat = 60.0 / tempo;

    rhythmIndex++;
    if (rhythmIndex == LOOP_LENGTH) {
        rhythmIndex = 0;
    }

    //0.25 because each square is a 16th note
    noteTime += 0.25 * secondsPerBeat;
}
</code></pre>

<p>Okay, there&rsquo;s a lot going on here, so let&rsquo;s break this down. Note that I didn&rsquo;t include anything about playing notes or updating the visuals as I just wanted to focus on the timing/scheduling.</p>

<p>First, in handlePlay(), we reset noteTime, which represents the length of a note. And then we set startTime, which includes an offset to take into account stuff happening in the main Javascript execution time (e.g. garbage collection).</p>

<p>Then, in schedule, we first check if we need to sequence audio events, depending on noteTime and currentTime. Note that the 200 ms offset here represents the variable scheduleAheadTime in the previous example. The while loop inside of the function schedule() is run again and again until we&rsquo;ve scheduled all notes that fall within our lookahead interval. Therefore, as we schedule more notes, and increment noteTime based on the tempo of the song (in advanceNote()):</p>

<pre><code class="javascript">var secondsPerBeat = 60.0 / tempo;
//...
noteTime += 0.25 * secondsPerBeat;
</code></pre>

<p>Since we&rsquo;re assuming a 4/4 time signature, to obtain noteTime we multipy secondsPerBeat by 0.25 since each note is a 16th note. Once we&rsquo;ve scheduled all of the notes that fall within our lookahead interval, the condition of the while loop in schedule()</p>

<pre><code class="javascript">while (noteTime &lt; currentTime + 0.200) {
</code></pre>

<p>will fail. Lastly, what&rsquo;s with the last line of schedule()?</p>

<pre><code class="javascript">timeoutId = setTimeout("schedule()", 0);
</code></pre>

<p>setTimeout is a Javascript library function that executes a function after a certain period of time. Therefore, this tells Javascript to call the schedule() function after 0 ms. So basically, call schedule() again right after it&rsquo;s scheduled all notes within out lookahead interval and begin the process over again.</p>

<p>Lastly, there&rsquo;s the rhythmIndex variable in advanceNote():</p>

<pre><code class="javascript">rhythmIndex++;
if (rhythmIndex == LOOP_LENGTH) {
    rhythmIndex = 0;
}
</code></pre>

<p>This variable simply represents where you are in the context of the loop, and resets when the sequence reaches then end so that it loops.</p>

<p>Cool, so now we can sequence audio properly! Well, kind of. As is pointed out in <a href="http://stackoverflow.com/questions/20598147/perfect-synchronization-with-web-audio-api">this StackOverflow</a>, synchronizing to the Web Audio clock does not mean that your audio is synchronized to the animation frame refresh rate. However, this is only important if you want to synchronize animations to Web Audio. For that, you can read the last section in <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/">A Tale of Two Clocks</a>.</p>

<p>If you&rsquo;ve got this scheduling thing down, and want to learn how to integrate it into our own application, I recommend checking out my code at <a href="https://github.com/catarak/web-audio-sequencer">Github</a> to see how I did it.</p>

<p>Thanks for reading, and look out for a part two of this blog post, which will explain more in depth about <strong>audio routing graphs</strong>, and different types of nodes, i.e. how to add different types of effects.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I Built a Drum Machine]]></title>
    <link href="http:///blog/2014/11/30/i-built-a-drum-machine/"/>
    <updated>2014-11-30T18:51:53-05:00</updated>
    <id>http:///blog/2014/11/30/i-built-a-drum-machine</id>
    <content type="html"><![CDATA[<p>In my college career, I majored in electrical engineering, and because of this, I learned about signal processing and audio. Now that I&rsquo;m learning web development, I&rsquo;ve been eager to combine my skills and delve into Web Audio, an API that allows you to do audio processing magic in the browser. This API allows you to create websites like Soundcloud or even <a href="http://echonest.github.io/remix/">remix songs algorithmically</a>. I decided to create a basic drum machine, and then modify it to create a digital sequencer.</p>

<p>I first followed this <a href="http://www.sitepoint.com/html5-web-audio-api-tutorial-building-virtual-synth-pad/">Virtual Synth</a> tutorial, which was a great place to start, but there were a few things I didn&rsquo;t like about it:</p>

<ul>
<li>No ability to create a sequence. What good is a drum machine without a sequencer (<a href="https://www.youtube.com/watch?v=wXRnbS6o64U">unless you&rsquo;re Araabmuzik</a>)?</li>
<li>Either I implemented the filter wrong, which is very possible, or it doesn&rsquo;t work correctly. It doesn&rsquo;t use a logarithmic scale, however, which is definitely a big mistake.</li>
</ul>


<p>I threw a bunch of stuff together just to get a working app, but I think that my next step is definitely to pause for a moment and refactor.</p>

<p>I will write a tutorial in the future for this, but for now, since my brains hurts a lot, you can check out the current version of my <a href="http://web-audio-sequencer.herokuapp.com/">Web Audio Sequencer</a>. And, of course, check out <a href="https://github.com/catarak/web-audio-sequencer">my Github</a> if you want to look at the code (<a href="https://github.com/catarak/web-audio-sequencer/commit/3f4a2f08f7f84acfc85297a626ce9286a43034d6">latest commit as writing this post</a>).</p>

<p><img class="center <a" src="href="http://media.giphy.com/media/vFtWp05vBYnMQ/giphy.gif">http://media.giphy.com/media/vFtWp05vBYnMQ/giphy.gif</a>&#8221;></p>
]]></content>
  </entry>
  
</feed>
